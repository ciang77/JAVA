1. 构建RAG检索流程：设计用户提问+知识片段拼接生成增强型Prompt的流程，结合语义召回和上下文信息，实现企业私有知识问答体系，提升问答准确率。 
在我们的企业知识问答系统中，RAG（Retrieval-Augmented Generation）检索流程主要分为三个阶段。首先是 知识库构建：使用 Apache Tika、pdfplumber、python-docx 等工具解析
企业内部 Word、PDF、TXT 等文档，并利用向量化模型（如 Sentence-BERT 或 OpenAI Embeddings）将文档切分为段落级向量，向量存入 FAISS 或 Milvus，原文和元数据存入 Elasticsearch，
用于语义和关键词联合检索。其次是 检索与召回：用户提问时，Spring Boot 后端将问题向量化，然后在向量库中执行 Top-K 相似度搜索，同时结合 Elasticsearch 的 bool should 查询进行关键词匹配，
保证语义覆盖和精确命中。最后是 增强型 Prompt 构建：系统将检索到的知识片段与用户提问拼接成增强型 Prompt，控制上下文长度后传递给大语言模型生成回答。整个服务通过 Spring Boot 提供 REST API 接口，
结合 Kafka 异步处理文件上传和向量化任务，Redis 做本地缓存加速查询，形成了低延迟、高准确率的企业私有知识问答体系。


2. 基于Kafka构建文件上传、解析与向量化的异步处理流程，实现模块解耦。经测试，500MB文档入队处理时间仅约200毫秒。 
在我们的系统中，文件上传、解析和向量化是通过 Kafka 构建的异步处理流程来实现模块解耦的。用户上传文档时，Spring Boot 后端首先将文件存储到对象存储（如 S3 或 MinIO），然后生成一个处理任务写入 Kafka 队列，
入队过程大约 200 毫秒即可完成，这样用户可以立即得到“任务提交成功”的响应，而无需等待整个文档解析和向量化完成。后台消费者服务异步从 Kafka 拉取任务，执行文档解析（使用 Apache Tika、pdfplumber 或 python-docx）、
文本切分和向量化（Sentence-BERT 或 OpenAI Embeddings），再将向量存入 FAISS 或 Milvus，原文和元数据存入 Elasticsearch。整个流程结合 Redis 缓存加速热点查询，实现了上传、解析和向量化的完全解耦，不仅提升了系统吞吐量和可扩展性，
也保证了用户的低延迟体验，同时支持大规模文档的异步处理。

3. 基于Redis实现会话历史管理，每个用户分配唯一会话ID，按时间顺序存储对话记录，设置7天过期，结合持久化机制支持断点恢复，保证多轮问答上下文连续性。 
在我们的系统中，会话历史管理是基于 Redis 实现的。具体做法是：当用户发起新对话时，后端生成一个唯一的 sessionId 作为标识，并以此在 Redis 中创建会话存储空间。每条对话会被序列化为 JSON 格式，包含用户角色、时间戳和消息内容，
然后按时间顺序追加到 Redis 的 List 结构中，同时为该 Key 设置 7 天过期时间，避免历史数据无限积累。用户再次提问时，系统会通过 LRANGE 命令取出最近若干条记录，拼接到 LLM 的 Prompt 中，从而保持多轮问答的上下文连续性。
为了保证数据可靠性，Redis 开启了 AOF 和 RDB 持久化，即使出现宕机也能恢复会话记录；同时我们还结合本地缓存，对高频访问的 session 做快速读取。整个实现基于 Spring Boot 与 Spring Data Redis，序列化采用 Jackson，前端与后端通过 
SSE 交互。这样设计的优点是：读写性能高，可以支撑大规模并发；数据可恢复，支持用户断点续聊；存储与业务逻辑解耦，易于扩展和维护。


4. 利用SSE与LLM Stream API，实现逐字流式返回结果，首响应时间从2.3秒降至0.6秒，显著提升用户交互体验。
在后端调用大模型时，不是等模型生成完整答案再返回，而是通过 LLM 的 Stream API（例如 OpenAI/本地模型的流式接口）逐步接收生成内容。
后端再通过 Server-Sent Events（SSE） 推送到前端，前端像接收数据流一样一边收到一边渲染。
这样用户会先看到前几个字，然后内容逐渐展开，而不是一次性卡几秒钟，如果说回答有误的话可以直接停止问答，
首响应更快：用户 0.6 秒左右就能看到结果，比传统的 2.3 秒等待要流畅得多。
交互体验提升：像和人对话一样，模型“边思考边说话”，用户等待的心理压力降低。
可中断/控制：流式返回时可以支持“停止生成”等操作，提高交互灵活性。
更贴近实时应用：适用于在线答题、客服、写作辅助等场景。


5. 集成Elasticsearch + FAISS，实现“关键词+语义”双引擎搜索，文档向量化后结合ES的bool should查询实现关键词与语义联合检索，支持多种文档格式（Word、PDF、TXT），显著提升搜索精度与召回效率。 
在企业知识检索中，我们采用了 Elasticsearch 与 FAISS 的双引擎方案来提升搜索精度与召回率。具体实现上，首先通过 Python 的文本解析工具（如 Apache Tika 或 pdfplumber）对 Word、PDF、TXT 等多种文档格式进行解析，
再利用向量化模型（如 Sentence-BERT 或 Hugging Face Embeddings）生成语义向量，并将原始文本索引存入 Elasticsearch，同时将向量存入 FAISS。查询时，用户的问题会被同时拆分为两部分：关键词检索和语义检索。
关键词部分利用 Elasticsearch 的 bool should 查询，快速匹配倒排索引中的相关文档；语义部分则通过向量化后在 FAISS 中进行相似度搜索，找到语义最接近的内容。最终我们将两类结果进行加权融合，返回给用户。
这样既能保证关键词匹配的精准性，又能通过语义检索覆盖语义相近但关键词不完全一致的场景，从而显著提升搜索的准确率与召回效率，同时支持大规模文档的低延迟查询。


6. 使用Redis有序集合管理设备能耗数据的时间序列，实现毫秒级排序与区间聚合分析；结合本地缓存和批量数据拉取技术，优化接口数据访问路径，将接口响应时间从180ms降至35ms，大幅提升系统查询效率和用户体验。
我们把能耗数据按 ZADD key timestamp value 的形式存到 Redis 有序集合，利用时间戳作为 score，就能天然保证有序存储和毫秒级范围查询。查询时直接用 ZRANGEBYSCORE 获取指定时间区间的数据，再在应用层做聚合计算。
为了优化延迟，我引入了 本地缓存（比如 Caffeine） 存储热点区间数据，同时对 Redis 查询做 批量拉取，避免多次网络 IO。最终接口响应从 180ms 优化到 35ms，在高并发下依然能保持稳定，并且支持秒级的区间分析。


1. 自主设计并实现Spring Cloud Gateway拦截器，采样所有接口调用耗时并上报到Spring Boot Admin，实现对关键接口及下游子系统响应时间的实时监控和告警。该机制能够及时发现子系统接口性能瓶颈，避免首页聚合或关键功能接口被慢接口拖慢，
保障系统整体响应速度和稳定性，同时为后续性能优化提供数据依据。
在我们的系统中，我自主设计并实现了 Spring Cloud Gateway 拦截器，用于采集所有接口的调用耗时并上报到 Spring Boot Admin 平台。拦截器在每个请求入口记录开始和结束时间，将耗时、请求路径、状态码等信息封装并异步上报，实现对关键接口及下游子系统的实时监控。
如果某个接口变慢，系统会通过告警及时通知运维或开发人员，同时可以触发限流或降级策略，避免慢接口拖慢首页聚合或关键功能接口，从而保障整体响应速度和稳定性。此外，这套机制生成的历史性能数据可用于分析瓶颈、优化接口和评估改进效果。
整个方案基于 Spring Cloud Gateway + Spring Boot Admin + 异步上报 实现，高效且可扩展。

针对接口变慢，我会在网关层结合限流、超时和降级策略处理，同时对耗时任务使用异步队列，必要时加缓存，这样既保护下游服务，又保证关键功能可用，并提升整体响应效率

2. 在作战指挥系统中负责项目与任务审批流相关功能的优化改造，采用Rocket MQ实现关键业务消息的异步处理与系统间解耦，有效提升系统响应效率与服务稳定性；同时参与导出报告功能的开发与维护，针对格式不规范等问题进行代码修复与样式优化，
确保导出内容符合业务需求与用户展示标准。 
在我们的系统中，关键业务消息的异步处理是通过 RocketMQ + Spring Boot 实现的。具体来说，业务模块在触发事件时，将消息发送到 RocketMQ 主题（Topic），消费者服务异步接收并处理这些消息，从而实现模块之间的 解耦，避免同步调用阻塞主流程，
提高系统响应效率和服务稳定性。对于导出报告功能，我参与了整个开发与维护流程：在生成 Excel、PDF 等报表时，处理格式不规范、样式混乱的问题，对代码进行了优化和修复，保证导出的内容不仅符合业务逻辑，也符合用户展示标准。同时，结合异步消息处理机制，
报表生成任务可以异步排队处理，减轻系统高并发下的压力，确保用户体验和系统稳定性。

在我们的系统中，为了实现模块解耦，前端或业务模块不会直接处理耗时任务，比如报表生成或文档向量化，而是将这些任务封装成消息发送到 RocketMQ。后端消费者服务异步接收并处理这些消息，这样业务模块不需要关心任务的具体执行过程，也不会因为慢任务而被阻塞，
实现了模块之间的松耦合。这样设计的好处是系统更容易维护和扩展，即使某个模块出现故障，也不会直接影响其他模块，同时可以独立升级或扩展某个模块而不影响整体系统的稳定性。

在我们的系统中，为了应对高并发任务，我们采用消息队列实现削峰填谷。用户在高峰期触发的任务（如大文件上传或报表生成）不会直接同步处理，而是先发送到 RocketMQ 队列中排队，后端消费者根据自身处理能力异步取消息执行任务。这样瞬时高负载被缓冲起来，
系统可以平稳处理请求，避免资源被瞬间压垮，同时保证任务顺序和可靠性，从而提升系统稳定性和整体处理效率。

在我们的系统中，无论是 Kafka 还是 RocketMQ，都可以实现模块解耦和削峰填谷。模块解耦指业务模块只负责发送消息，不直接执行耗时任务，消费者异步处理，实现各模块松耦合；削峰填谷则是高峰请求先进入队列缓冲，消费者按自身处理能力异步处理，平滑高峰压力。
区别在于：Kafka 更适合大吞吐量和流式数据处理，适合日志或实时分析；RocketMQ 提供顺序消息、事务消息和延迟消息，更适合企业业务场景和复杂事务处理。整体上，两者都能保证系统稳定性、提高响应效率，同时支持高并发环境下可靠的任务处理。

3. 设计接口数据处理机制，解析前端JSON查询条件，自动定位事实表并动态生成SQL，关联维度表抽取完整数据返回前端报表；该方案避免手动拼接错误，支持灵活查询，提高接口效率与数据准确性，并降低维护成本。
在我们的系统中，接口数据处理机制基于 Spring Boot + MyBatis 动态 SQL + Jackson/ FastJSON 实现，实现了前端 JSON 查询条件到数据库查询的全流程自动化。具体来说，前端传入的 JSON 包含过滤字段、操作符、范围条件、排序规则和分页信息，
后端首先解析这些条件，然后根据业务元数据和表关系 自动定位事实表，判断需要关联哪些维度表来获取完整数据。接着，利用 MyBatis 动态 SQL 或 JPA Criteria API 生成安全、结构化的 SQL 查询语句，包括 INNER JOIN 或 LEFT JOIN 关联维度表，
并动态拼接 WHERE、ORDER BY、GROUP BY 等条件，确保查询结果与前端需求完全一致。查询完成后，将数据封装成前端报表所需结构，支持分页和聚合展示。整个方案避免了手动拼接 SQL 的风险，提高了接口效率和数据准确性，同时降低维护成本，并支持灵活扩展：
增加新维度字段或修改报表逻辑无需修改底层代码，只需更新元数据映射即可。

4. 针对内网环境下源数据接口首次调用慢的问题，引入Completable Future并行化请求多个模板字段，将接口响应时间从1秒以上优化至0.3秒。
在我们的系统中，针对内网环境下源数据接口首次调用响应较慢的问题，我们引入了 Java 的 CompletableFuture 并行化处理来优化接口性能。具体实现是：当后端 Spring Boot 服务需要获取多个模板字段的数据时，原先是顺序调用接口，
每个字段依赖网络请求，首次调用时间往往超过 1 秒。现在我们将每个字段的请求封装为一个 CompletableFuture，通过 CompletableFuture.supplyAsync() 异步执行多个请求，并使用 CompletableFuture.allOf() 等待所有异步结果返回，
然后再统一汇总返回给前端。这样多个接口请求可以同时发起，充分利用 CPU 与网络资源，实现并行化，避免阻塞等待。经过测试，首次调用接口的平均响应时间从原来的 1 秒以上降到约 0.3 秒，显著提升了用户体验，同时仍保证了调用结果的完整性和顺序可控。


5. 设计并实现基于Redis ZSet与Lua脚本的用户会话控制机制，限制单用户最多同时在线N个终端，自动淘汰最早的token，有效防止token滥用和系统资源过载，提升系统稳定性与安全性。
在这个功能里，Lua 脚本的角色就是：会话控制逻辑的“原子化执行单元”：在 Redis 内部一次性完成会话数检查 → 旧 token 淘汰 → 新 token 添加，避免并发冲突，减少网络延迟，提升性能和安全性。
注意：redis和lua脚本结合可能会出现脚本注入问题，在 Redis 里用 Lua，安全性取决于你是否把用户输入当作代码执行。只要所有用户数据都通过 KEYS / ARGV 参数传入，就不会有脚本注入的风险。


6. 负责引入Transmittable ThreadLocal（TTL）并实现上下文传递机制，将请求头中的用户信息缓存至线程变量中，支持在异步线程/线程池环境中安全传递用户上下文，减少重复数据库查询，提高系统性能与上下文访问效率。
利用 Java 的 CompletableFuture，对多个模板字段的请求进行异步并行处理。并行发起请求，等待全部完成后再汇总结果返回。减少等待时间，显著提升响应速度。同时，我配置了专用线程池和超时控制，确保并发量受控且具备容错能力。
最终将接口响应时间从1秒以上优化至0.3秒，大幅提升了系统性能和用户体验。

ThreadLocal 是什么？
ThreadLocal 是Java中 lang 包下的一个类，是用来解决多线程下共享变量并发问题的，所谓共享变量即同一个变量在不同线程下赋予不同值。
ThreadLocal 会在多线程环境中为每个线程维护独立的变量副本，让每个线程都拥有自己的数据副本，避免了多个线程同时访问同一个变量的冲突问题。
在 ThreadLocal 中每一个thread维护一个threadlocalmap，threadlocalmap是由threadlocal维护的，map里面存的key是threadlocal对象本身，value是变量副本，主要有set方法和get方法。
threadlocalmap 是threadlocal静态内部类，key是threadlocal对象是弱引用，目的是将threadlocal对象的生命周期和线程的生命周期解绑

内存泄漏指的是什么？他和内存溢出有什么区别？
内存泄漏是指无用对象无法被GC回收，始终占用内存，造成空间浪费，最终会导致内存溢出，内存溢出指的是程序申请内存，没有足够的空间供其使用，out of memory。

OK，那你清楚 threadlocal 出现内存泄漏的真实原因是什么吗？
一、没有手动删除Entry对象，使用完threadlocal调用其remove方法就可以删除对应的Entry，避免内存泄漏；


